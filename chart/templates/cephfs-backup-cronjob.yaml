apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: backup-volumes-cephfs
  namespace: {{ .Values.namespace }}
  annotations:
    description: "Nightly restic backup for CephFS volumes."
  labels:
    app: restic-backup
spec:
  schedule: {{ .Values.backup.schedule }}
  concurrencyPolicy: Allow
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    metadata:
      labels:
        app: restic-backup
    spec:
      activeDeadlineSeconds: 86400 #24h
      backoffLimit: 2
      # we are in the "parallel jobs with a work queue" scenario from https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/
      # thus do not specify `.spec.completions` and set `.spec.parallelism`
      parallelism: {{ .Values.backup.parallelism }}
      template:
        spec:
          serviceAccountName: {{ .Values.backup.serviceAccountName }}
          initContainers:
          - image: {{ .Values.backupCronjob.image }}
            imagePullPolicy: Always
            name:  enqueue-volumes-cephfs
            env:
            - name: REDIS_QUEUE_INIT_NAME
              value: {{ .Values.backupCronjob.redisQueueInitName }}
            command: [ "/enqueue_pvs.sh" ]
            volumeMounts:
            - name: jobinfo
              mountPath: /etc/jobinfo
              readOnly: true
          containers:
          - image: {{ .Values.backupCronjob.image }}
            imagePullPolicy: Always
            name:  backups-volumes-cephfs
            securityContext:
              privileged: true
              runAsUser: 0
            volumeMounts:
            - name: cache
              mountPath: /.cache
            - name: jobinfo
              mountPath: /etc/jobinfo
              readOnly: true
            env:
            # init name to initialize redis queue and distinguish from others
            - name: REDIS_QUEUE_INIT_NAME
              value: {{ .Values.backupCronjob.redisQueueInitName }}
            # env variables we need for restic backup
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  key: cephfs-backup-s3-access-key
                  name: cephfs-backup-secrets
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  key: cephfs-backup-s3-secret-key
                  name: cephfs-backup-secrets
            - name: RESTIC_REPO_BASE
              valueFrom:
                secretKeyRef:
                  key: cephfs-backup-repository-base
                  name: cephfs-backup-secrets
            - name: RESTIC_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: cephfs-backup-password
                  name: cephfs-backup-secrets
            # env variable we need to query manila shares in OpenStack
            - name: OS_USER_NAME
              value: {{ .Values.openstack.username }}
            - name: OS_CREDENTIAL_ID
              valueFrom:
                secretKeyRef:
                  key: os-applicationCredentialID
                  name: {{ .Values.openstack.manilaSecretName }}
            - name: OS_CREDENTIAL_SECRET
              valueFrom:
                secretKeyRef:
                  key: os-applicationCredentialSecret
                  name: {{ .Values.openstack.manilaSecretName }}
            # arguments for scrape backup pods by prometheus pushgateway
            - name: pushgateway_service
              value: http://{{ .Values.backupMetrics.serviceName }}.{{ .Values.namespace }}.svc:9091
            command: [ "/backup_pvs.sh" ]
            lifecycle:
              preStop:
                exec:
                  # In case the pod gets terminated (e.g. we reach the job's activeDeadlineSeconds),
                  # send ctrl-c and wait for main process to perform cleanup and exit cleanly.
                  command: [ "/bin/sh", "-c", "killall -INT restic; sleep 30" ]
          volumes:
          - name: cache
            emptyDir: {}
          # Get job name uid through the downward API.
          # It is required to run parallel pods in the job and be able to do
          # simultaneously backups in parallel of different PVs.
          - name: jobinfo
            downwardAPI:
              items:
                - path: "labels"
                  fieldRef:
                    fieldPath: metadata.labels
          # We user Never because we prefer to start a new container when failure.
          # This will start from scratch for the rest of the PVs to backup.
          restartPolicy: Never
          nodeSelector:
{{ .Values.nodeSelector | toYaml | indent 12 }}
