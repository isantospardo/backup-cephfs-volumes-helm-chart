apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: forget-backup-volumes-cephfs
  namespace: {{ .Values.namespace }}
  annotations:
    description: "Weekly restic backup forget for CephFS volumes."
  labels:
    app: restic-backup-forget
spec:
  schedule: {{ .Values.backupForget.schedule }}
  concurrencyPolicy: Allow
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    metadata:
      labels:
        app: restic-backup
    spec:
      activeDeadlineSeconds: 86400 #24h
      backoffLimit: {{ .Values.backupForget.retries }}
      # we are in the "parallel jobs with a work queue" scenario from https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/
      # thus do not specify `.spec.completions` and set `.spec.parallelism`
      parallelism: {{ .Values.backupForget.parallelism }}
      template:
        spec:
          serviceAccountName: {{ .Values.backupForget.serviceAccountName }}
          initContainers:
          - image: {{ .Values.backupCronjob.image }}
            name:  enqueue-volumes-cephfs
            env:
            - name: REDIS_QUEUE_INIT_NAME
              value: {{ .Values.backupForget.redisQueueInitName }}
            command: [ "/enqueue_pvs.sh" ]
            volumeMounts:
            - name: jobinfo
              mountPath: /etc/jobinfo
              readOnly: true
          containers:
          - image: {{ .Values.backupCronjob.image }}
            name:  forget-backups-volumes-cephfs
            volumeMounts:
            - name: cache
              mountPath: /.cache
            - name: jobinfo
              mountPath: /etc/jobinfo
              readOnly: true
            env:
            # init name to initialize redis queue and distinguish from others
            - name: REDIS_QUEUE_INIT_NAME
              value: {{ .Values.backupForget.redisQueueInitName }}
            # `restic check` does not use the /.cache folder by default
            # as per https://forum.restic.net/t/restic-0-9-0-check-fills-up-my-tmp-partition/679/9
            # We must explicitly point TMPDIR to the emptyDir, otherwise `restic check` will fill up
            # the container storage.
            - name: TMPDIR
              value: /.cache
            # env variables we need for restic backup
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  key: cephfs-backup-s3-access-key
                  name: cephfs-backup-secrets
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  key: cephfs-backup-s3-secret-key
                  name: cephfs-backup-secrets
            - name: RESTIC_REPO_BASE
              valueFrom:
                secretKeyRef:
                  key: cephfs-backup-repository-base
                  name: cephfs-backup-secrets
            - name: RESTIC_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: cephfs-backup-password
                  name: cephfs-backup-secrets
            # arguments for scrape forget backup pods by prometheus pushgateway
            - name: pushgateway_service
              value: http://{{ .Values.backupMetrics.serviceName }}.{{ .Release.Namespace }}.svc:9091
            # arguments for restic forget
            - name: restic_forget_args
              value: {{ .Values.backupForget.resticForgetArgs }}
            command: [ "/forget_backup_pvs.sh" ]
            lifecycle:
              preStop:
                exec:
                  # In case the pod gets terminated (e.g. we reach the job's activeDeadlineSeconds),
                  # send ctrl-c and wait for main process to perform cleanup and exit cleanly.
                  command: [ "/bin/sh", "-c", "killall -INT restic; sleep 30" ]
          volumes:
          - name: cache
            emptyDir: {}
          # Get job name uid through the downward API.
          # It is required to run parallel pods in the job and be able to do
          # simultaneously forget backups in parallel of different PVs.
          - name: jobinfo
            downwardAPI:
              items:
                - path: "labels"
                  fieldRef:
                    fieldPath: metadata.labels
          # We use Never because of the retry policy we are using,
          # with never it will recreate the pod to avoid to use the older emptyDir/hostPath.
          # There are improvements in 1.12 (in OKD4+ we could use OnFailure and
          # restart the container in the same pod rather than Never, which starts a new pod)
          # https://github.com/kubernetes/kubernetes/issues/54870
          restartPolicy: Never
          nodeSelector:
{{ .Values.nodeSelector | toYaml | indent 12 }}
